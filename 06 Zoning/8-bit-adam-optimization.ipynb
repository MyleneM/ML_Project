{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 8-bit Adam Optimization ðŸ‘¾\n\n#### The optimizer is responsible for computing the gradient statistics for back propagation. These calculations are typically done on 32-bit values, but this notebook demonstrates how to use an 8-bit optimizer that saves memory and increases speed.\n\n#### The problem with reducing the number of bits is that the precision of each value decreases. Tim Dettmers ([@timdettmers](https://www.kaggle.com/timdettmers)) did research at Facebook to figure out how to do stable optimization using 8 bits using a clever quantization trick. For a more detailed look at his research, please [read his paper](https://arxiv.org/abs/2110.02861) or [view his humorous video](https://www.youtube.com/watch?v=IxrlHAJtqKE). The GitHub repo, which contains installation instructions for your specific GPU, can be found here: https://github.com/facebookresearch/bitsandbytes \n\n#### It was found that this allows for slightly faster training and for slightly larger models to be loaded into memory without sacrificing performance. \n\n#### In this notebook, I compare the training times between the regular 32-bit Adam and the 8-bit Adam optimizer when training longformer-large for 1 epoch using a maximum sequence length of 2048. To use 8-bit Adam, you need to install the library and then change the one [line where the optimizer gets created](#Optimizer). In some cases using 8-bit Adam allows for larger batch sizes. Not this time, though ðŸ˜…\n\n#### If you want to jump straight to the results, [click here](#Weights-and-Biases-Report-âœ¨)\n\n#### If you want to see the notebook during the actual runs, version 1 has 8-bit Adam and version 3 has 32-bit Adam.\n\n#### I think 8-bit Adam is mostly useful for training large language models from scratch, and less for finetuning models with < 1B parameters. Perhaps the best use-case for Kaggle would be for the users who don't have any other compute and batch size of 1 just barely doesn't fit using 32-bit Adam. In that instance, 8-bit Adam would allow people to use Kaggle GPUs to train models that wouldn't fit otherwise.\n\n#### The speed-ups are modest as seen in this image from the paper below.\n![8bit speed table](https://pbs.twimg.com/media/FBLeOZnVEAkt-Ij?format=png&name=small)\n\n#### 8 bit optimization also enables fitting bigger models on smaller GPUs: \n![8bit fit big models](https://pbs.twimg.com/media/FBLeMS_VIBEettR?format=png&name=900x900)\n\n\n#### [Tim's announcement tweet](https://twitter.com/Tim_Dettmers/status/1446472128979562499?s=20) ","metadata":{}},{"cell_type":"markdown","source":"# Install necessary libraries ðŸ“š\n\nYou must install the right version of `bitsandbytes` according to the GPU's CUDA version.","metadata":{}},{"cell_type":"code","source":"# Take note of what cuda version you have by running either of the following commands\n# !conda list | grep cudatoolkit\n!nvidia-smi\n\n# choices: {cuda92, cuda 100, cuda101, cuda102, cuda110, cuda111, cuda113}\n# replace XXX with the respective number\n# pip install bitsandbytes-cudaXXX\n!pip install bitsandbytes-cuda110 -q\n!pip install -U wandb -q\n!pip install seqeval git+https://github.com/huggingface/transformers.git -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This tests if the installation was successful\n!wget https://gist.githubusercontent.com/TimDettmers/1f5188c6ee6ed69d211b7fe4e381e713/raw/4d17c3d09ccdb57e9ab7eca0171f2ace6e4d2858/check_bnb_install.py && python check_bnb_install.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport math\nfrom pathlib import Path\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport datasets\nfrom transformers import (\n    TrainingArguments, \n    Trainer, \n    AutoConfig,\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    get_scheduler,\n    DataCollatorForTokenClassification,\n)\nimport bitsandbytes as bnb","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    \n    fold = 0\n    \n    model_name = \"allenai/longformer-large-4096\"\n    \n    max_seq_length = 2048\n    text_column = \"text\"\n    label_column = \"labels\"\n    word_id_column = \"word_ids\"\n\n    training_args = TrainingArguments(\n        output_dir=\"lf_2k\",\n        overwrite_output_dir=True,\n        do_train=True,\n        do_eval=True,\n        evaluation_strategy=\"epoch\",\n        per_device_train_batch_size=1,\n        per_device_eval_batch_size=16,\n        gradient_accumulation_steps=8,\n        learning_rate=3e-5,\n        weight_decay=0.01,\n        num_train_epochs=1,\n        max_steps=-1, # set >0 to limit\n        lr_scheduler_type=\"linear\",\n        warmup_ratio=0.1,\n        logging_strategy=\"steps\",\n        logging_steps=100,\n        save_strategy=\"epoch\",\n        save_steps=None,\n        seed=18,\n        fp16=True, \n        eval_steps=None, # change evaluation_strategy to steps to use this\n        dataloader_num_workers=2,\n        run_name=\"longformer-2k-8bit-test\",\n        group_by_length=True, # This can also help speed training\n        report_to=\"wandb\",\n        resume_from_checkpoint=None,\n    )\n\n# for convenience\nargs = CFG.training_args","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I use weights and biases to track training.\n# The following code requires attaching a secret to the notebook.\nif \"wandb\" in args.report_to:\n    import wandb\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    key = user_secrets.get_secret(\"wandb\")\n    \n    wandb.login(key=key)\n    os.environ[\"WANDB_PROJECT\"] = \"feedback-prize\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create dataset\n\nThis is a little slow so it would probably be wise to create your dataset in another notebook and then load it in to the training notebook.","metadata":{}},{"cell_type":"code","source":"%%time\n\nif not Path(\"full_dataset.dataset\").exists():\n    texts, ids = [], []\n    for file in tqdm(Path(\"../input/feedback-prize-2021/train\").glob(\"*.txt\"), total=15594, desc=\"Reading train texts\"):\n        ids.append(file.stem)\n\n        with open(file) as fp:\n            texts.append(fp.read())\n        \n    \ndef add_label_information(examples):\n    \n    texts = examples[CFG.text_column]\n    ids = examples[\"id\"]\n    all_labels, folds, words = [], [], []\n    \n    for text, id_ in zip(texts, ids):\n    \n        df = train_df[train_df[\"id\"]==id_]\n\n        text = text.split()\n        num_words = len(text)\n\n        labels = [\"O\"]*num_words\n\n        for discourse_type, predictionstring in df[[\"discourse_type\", \"predictionstring\"]].values:\n\n            first = True\n            for word_id in map(int, predictionstring.split()):\n                prefix = \"I-\"\n                if first:\n                    prefix = \"B-\"\n                    first = False\n                labels[word_id] = prefix+discourse_type\n                \n        all_labels.append(labels)\n        folds.append(df[\"kfold\"].values[0])\n        words.append(text)\n\n    examples[CFG.label_column] = all_labels\n    examples[\"fold\"] = folds\n    examples[CFG.text_column] = words\n    return examples\n    \n\n# Using fold strategy shown by Abhishek https://www.kaggle.com/abhishek/creating-folds-properly-hopefully-p/\ntrain_df = pd.read_csv(\"../input/creating-folds-properly-hopefully-p/train_folds.csv\", usecols=[\"id\", \"discourse_type\", \"predictionstring\", \"kfold\"])\n\n\n# This step can take a few minutes\nif not Path(\"full_dataset.dataset\").exists():\n    temp_dataset = datasets.Dataset.from_dict({\"id\": ids, CFG.text_column: texts})\n    temp_dataset = temp_dataset.map(add_label_information, batched=True, num_proc=args.dataloader_num_workers)\n\n    full_dataset = datasets.DatasetDict()\n    full_dataset[\"train\"] =  temp_dataset.filter(lambda x: x[\"fold\"]!=CFG.fold)\n    full_dataset[\"validation\"] =  temp_dataset.filter(lambda x: x[\"fold\"]==CFG.fold)\n    full_dataset.save_to_disk(\"full_dataset.dataset\")\nelse:\n    full_dataset = datasets.DatasetDict.load_from_disk(\"full_dataset.dataset\")\nfull_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizing data\n\nDon't pad to max length unless you are on a TPU or you really want to extend your training","metadata":{}},{"cell_type":"code","source":"# https://github.com/huggingface/transformers/blob/669e3c50c98ad5b506555a551d2ecbf72ceb3c99/examples/pytorch/token-classification/run_ner.py#L371\ndef tokenize_and_align_labels(examples, label2id, return_word_ids=False):\n    tokenized_inputs = tokenizer(\n        examples[CFG.text_column],\n        truncation=True,\n        max_length=CFG.max_seq_length,\n        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n        is_split_into_words=True,\n    )\n    labels = []\n    all_word_ids = []\n    for i, label in enumerate(examples[CFG.label_column]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n            # ignored in the loss function.\n            if word_idx is None:\n                label_ids.append(-100)\n            else:\n                label_ids.append(label2id[label[word_idx]])\n            previous_word_idx = word_idx\n            \n        if return_word_ids:\n            all_word_ids.append(word_ids)\n\n        labels.append(label_ids)\n    \n    tokenized_inputs[CFG.label_column] = labels\n    \n    if return_word_ids:\n        tokenized_inputs[CFG.word_id_column] = all_word_ids\n    \n    return tokenized_inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nlabel_list = ['O', 'B-Claim', 'I-Claim', 'B-Concluding Statement', 'I-Concluding Statement', \n              'B-Counterclaim', 'I-Counterclaim', 'B-Evidence', 'I-Evidence','B-Lead', 'I-Lead', \n              'B-Position', 'I-Position', 'B-Rebuttal', 'I-Rebuttal']\n\nlabel2id = {label:id_ for id_, label in enumerate(label_list)}\nid2label = {id_:label for id_, label in enumerate(label_list)}\n\ntokenizer = AutoTokenizer.from_pretrained(CFG.model_name, add_prefix_space=True)\n\ntrain_dataset = full_dataset[\"train\"].map(\n        partial(\n            tokenize_and_align_labels,\n            label2id=label2id,\n            return_word_ids=False\n    ),\n    batched=True,\n    num_proc=args.dataloader_num_workers,\n    remove_columns=[\"fold\", \"text\", \"id\"]\n)\n\n    \nvalidation_dataset = full_dataset[\"validation\"].map(\npartial(\n    tokenize_and_align_labels,\n    label2id=label2id,\n    return_word_ids=True\n),\nbatched=True,\nnum_proc=args.dataloader_num_workers,\n    remove_columns=[\"fold\"]\n)\n\n# bonus points if you can explain why it says\n# Ignored unknown kwarg option direction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_config = AutoConfig.from_pretrained(\n    CFG.model_name,\n    num_labels=len(label_list),\n    label2id=label2id,\n    id2label=id2label,\n    finetuning_task=\"ner\",\n)\n\nmodel = AutoModelForTokenClassification.from_pretrained(CFG.model_name, config=model_config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimizer\n\nHere is the key cell where the 8-bit Adam optimizer gets set. It's pretty much trivially easy...","metadata":{}},{"cell_type":"code","source":"no_decay = [\"bias\", \"LayerNorm.weight\"]\noptimizer_grouped_parameters = [\n    {\n        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n        \"weight_decay\": args.weight_decay,\n    },\n    {\n        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n        \"weight_decay\": 0.0,\n    },\n]\n# This one line is all that is need to run 8-bit Adam\noptimizer = bnb.optim.Adam8bit(optimizer_grouped_parameters, lr=args.learning_rate)\n\n\nnum_update_steps_per_epoch = len(train_dataset) // args.per_device_train_batch_size // args.gradient_accumulation_steps\nif args.max_steps == -1 or args.max_steps is None:\n    args.max_steps = args.num_train_epochs * num_update_steps_per_epoch\nelse:\n    args.num_train_epochs = math.ceil(args.max_steps / num_update_steps_per_epoch)\n    \nif args.warmup_ratio is not None:\n    args.num_warmup_steps = int(args.warmup_ratio * args.max_steps)\n\nlr_scheduler = get_scheduler(\n    name=args.lr_scheduler_type,\n    optimizer=optimizer,\n    num_warmup_steps=args.num_warmup_steps,\n    num_training_steps=args.max_steps,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Collator and Metrics ðŸ“ \n\nThis collator is really handy because I can tell it to pad to a multiple of a number. Longformer likes to have inputs in multiples of 512, so it will handle the padding for me!","metadata":{}},{"cell_type":"code","source":"# Data collator\npad_to_multiple_of = 512 # this is for longformer, use 1024 for bigbird\n    \ndata_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n\n# Metrics\nmetric = datasets.load_metric(\"seqeval\")\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CV calculation functions","metadata":{}},{"cell_type":"code","source":"# Rob Mulla @robikscube\n# https://www.kaggle.com/robikscube/student-writing-competition-twitch\ndef calc_overlap(pred, ground_truth):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(pred.split(' '))\n    set_gt = set(ground_truth.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter / len_gt\n    overlap_2 = inter/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given discourse_type are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','discourse_type'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = [calc_overlap(pred, gt) for pred, gt in joined[['predictionstring_pred', 'predictionstring_gt']].values]\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    denominator = (TP + 0.5*(FP+FN))\n    if denominator == 0:\n        return 0.0\n    my_f1_score = TP / denominator\n    return {\n        \"F1\": round(my_f1_score, 4),\n        \"Precision\": TP/(TP+FP),\n        \"Recall\": TP/(TP+FN), # This was calculated incorrectly in the runs\n    }\n        \n\nid2label={i: l for l, i in label2id.items()}\n# https://www.kaggle.com/zzy990106/pytorch-ner-infer?scriptVersionId=82677278&cellId=13\ndef get_label_predictions(dataset, preds):\n\n    ids = dataset[\"id\"]\n    word_ids = dataset[CFG.word_id_column]\n    words = dataset[CFG.text_column]\n    \n    all_preds = []\n\n    for id_, sample_preds, sample_word_ids, words in zip(ids, preds, word_ids, words):\n        label_preds = [\"\"]*len(words)\n\n        for pred, w_id in zip(sample_preds, sample_word_ids):\n            if w_id is None:\n                continue\n            if label_preds[w_id] == \"\":\n                label_preds[w_id] = id2label[pred]\n\n        j = 0\n        while j < len(label_preds):\n            label = label_preds[j]\n\n            if label.startswith(\"B\"):\n                label = label.replace(\"B\", \"I\")\n                end = j + 1\n                while end < len(label_preds) and label_preds[end] == label:\n                    end += 1\n\n                if end - j > 7:\n                    all_preds.append((id_, label.lstrip(\"BI-\"), ' '.join(map(str, list(range(j, end))))))\n\n                j = end\n            else:\n                j += 1\n                \n    return all_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create custom trainer","metadata":{}},{"cell_type":"code","source":"class FeedbackPrizeTrainer(Trainer):\n    \n    def __init__(self, *args, **kwargs):\n        # The Trainer will remove the important columns needed for cv from the eval_dataset,\n        # so we'll just store it like this\n        if \"cv_dataset\" in kwargs:\n            self.cv_dataset = kwargs.pop(\"cv_dataset\")\n        super().__init__(*args, **kwargs)\n        \n        \n    def evaluation_loop(\n        self, \n        dataloader,\n        description,\n        prediction_loss_only = None,\n        ignore_keys = None,\n        metric_key_prefix = \"eval\",\n    ):\n        \n        eval_output =  super().evaluation_loop(\n            dataloader,\n            description,\n            prediction_loss_only,\n            ignore_keys,\n            metric_key_prefix\n        )\n        \n        # Custom CV F1 calculation\n        # This same loop gets called during predict, and we can't do CV when predicting\n        if metric_key_prefix == \"eval\":\n            \n            eval_id_preds = eval_output.predictions.argmax(-1)\n            eval_label_preds = get_label_predictions(self.cv_dataset, eval_id_preds)\n            \n            eval_pred_df = pd.DataFrame(eval_label_preds, columns=[\"id\", \"discourse_type\", \"predictionstring\"])\n            \n            eval_gt_df = train_df[train_df[\"id\"].isin(self.cv_dataset[\"id\"])].reset_index(drop=True).copy()\n            \n            classes = ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement', 'Counterclaim', 'Rebuttal']\n            f1_scores = []\n            for class_ in classes:\n                gt_df = eval_gt_df.loc[eval_gt_df['discourse_type'] == class_].copy()\n                pred_df = eval_pred_df.loc[eval_pred_df['discourse_type'] == class_].copy()\n                eval_scores = score_feedback_comp(pred_df, gt_df)\n                for score_name, score in eval_scores.items():\n                    eval_output.metrics[f\"{metric_key_prefix}_{class_}_CV_{score_name}\"] = score\n                f1_scores.append(eval_scores[\"F1\"])\n                \n            eval_output.metrics[f\"{metric_key_prefix}_Overall_CV_F1\"] = np.mean(f1_scores)\n        \n        return eval_output\n\n# Initialize our Trainer\ntrainer = FeedbackPrizeTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    cv_dataset=validation_dataset, \n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    optimizers=(optimizer, lr_scheduler)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train! ðŸš†","metadata":{}},{"cell_type":"code","source":"%env TOKENIZERS_PARALLELISM=true\n\n\ntrain_result = trainer.train()\nmetrics = train_result.metrics\ntrainer.save_model()  # Saves the tokenizer too\n\nmetrics[\"train_samples\"] = len(train_dataset)\n\ntrainer.log_metrics(\"train\", metrics)\ntrainer.save_metrics(\"train\", metrics)\ntrainer.save_state()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Weights and Biases Report âœ¨\n\n#### You can see the lower memory usage and marginally faster training time. The loss curves are nearly identical and CV F1 scores are pretty much the same as well. \n\n<iframe src=\"https://wandb.ai/nbroad/feedback-prize/reports/8-bit-Adam-vs-32-bit-Adam--VmlldzoxNDQ5Nzg3\" style=\"border:none;height:1024px;width:100%\">","metadata":{}}]}